[
  {
    "id": "twitter_1989810610742464818",
    "source": "twitter",
    "author": "ChrisVanDerKlau",
    "content": "@hatchprop @FactoryAI Ok I thought by codex you meant the gpt5 codex model under Droid",
    "url": "https://x.com/ChrisVanDerKlau/status/1989810610742464818",
    "timestamp": "2025-11-15T21:39:53.000Z",
    "metadata": {
      "conversation_id": "1989810610742464818",
      "likes": 0,
      "retweets": 0,
      "replies": 0,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989804248121708791",
    "source": "twitter",
    "author": "sasuke___420",
    "content": "@fire @FactoryAI ü§îü§îü§îü§î",
    "url": "https://x.com/sasuke___420/status/1989804248121708791",
    "timestamp": "2025-11-15T21:14:36.000Z",
    "metadata": {
      "conversation_id": "1989804248121708791",
      "likes": 2,
      "retweets": 0,
      "replies": 0,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989802318758609211",
    "source": "twitter",
    "author": "moumensoliman",
    "content": "@FactoryAI 2000 !!",
    "url": "https://x.com/moumensoliman/status/1989802318758609211",
    "timestamp": "2025-11-15T21:06:56.000Z",
    "metadata": {
      "conversation_id": "1989802318758609211",
      "likes": 1,
      "retweets": 0,
      "replies": 0,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989800547969650837",
    "source": "twitter",
    "author": "robert_heimir",
    "content": "@hatchprop @FactoryAI Right there with ya - for 200 it takes effort to even hit the cap",
    "url": "https://x.com/robert_heimir/status/1989800547969650837",
    "timestamp": "2025-11-15T20:59:53.000Z",
    "metadata": {
      "conversation_id": "1989800547969650837",
      "likes": 1,
      "retweets": 0,
      "replies": 0,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989799575230845050",
    "source": "twitter",
    "author": "ben_vargas",
    "content": "@Mohanad423461 @FactoryAI nope, it works surprisingly well... I didn't think Droid would work as well as it does with this approach - I was honestly surprised because I haven't had the same success with opencode + proxy.",
    "url": "https://x.com/ben_vargas/status/1989799575230845050",
    "timestamp": "2025-11-15T20:56:02.000Z",
    "metadata": {
      "conversation_id": "1989799575230845050",
      "likes": 1,
      "retweets": 0,
      "replies": 0,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989797011093430730",
    "source": "twitter",
    "author": "dedxyz",
    "content": "@FactoryAI 5 more months until april sir.",
    "url": "https://x.com/dedxyz/status/1989797011093430730",
    "timestamp": "2025-11-15T20:45:50.000Z",
    "metadata": {
      "conversation_id": "1989797011093430730",
      "likes": 0,
      "retweets": 0,
      "replies": 0,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989796273768272206",
    "source": "twitter",
    "author": "Mohanad423461",
    "content": "@ben_vargas @FactoryAI one more thing im concerned about, the fact that the method is wrapping codex with more tools, do you find any issues and need reprompting as the codex tools and prompts are always in the requests",
    "url": "https://x.com/Mohanad423461/status/1989796273768272206",
    "timestamp": "2025-11-15T20:42:54.000Z",
    "metadata": {
      "conversation_id": "1989796273768272206",
      "likes": 0,
      "retweets": 0,
      "replies": 1,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989791963508564337",
    "source": "twitter",
    "author": "hatchprop",
    "content": "@ChrisVanDerKlau @FactoryAI no I am showing the codex cli with openai pro sub is 200/m for about 20B tokens at the limit. Droid would be 100x the cost.",
    "url": "https://x.com/hatchprop/status/1989791963508564337",
    "timestamp": "2025-11-15T20:25:47.000Z",
    "metadata": {
      "conversation_id": "1989791963508564337",
      "likes": 0,
      "retweets": 0,
      "replies": 1,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989790138205978729",
    "source": "twitter",
    "author": "ChrisVanDerKlau",
    "content": "@hatchprop @FactoryAI That's 6.5k for 30B Tokens (0.22$/M)\n\nAre you switching to the 2k 20B tier ? (0.10$/M)",
    "url": "https://x.com/ChrisVanDerKlau/status/1989790138205978729",
    "timestamp": "2025-11-15T20:18:32.000Z",
    "metadata": {
      "conversation_id": "1989790138205978729",
      "likes": 0,
      "retweets": 0,
      "replies": 1,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989789920156885218",
    "source": "twitter",
    "author": "MedonB082602",
    "content": "The different types of droids rlly make @FactoryAI  superior to Claude code. It does it so much better! Comment if you agree or what you think. \n#AI #startup #fyp",
    "url": "https://x.com/MedonB082602/status/1989789920156885218",
    "timestamp": "2025-11-15T20:17:40.000Z",
    "metadata": {
      "conversation_id": "1989789920156885218",
      "likes": 0,
      "retweets": 0,
      "replies": 0,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989787731694940161",
    "source": "twitter",
    "author": "mjs527",
    "content": "i blew through my @FactoryAI MAX plan in 3 days!!! my systems are working more effectively than ever now, but even with intense architecture optimization, progressive disclosure, tiered skills/mcp/subdroid usage, i‚Äôm still crushing through my limits. really need this upgrade",
    "url": "https://x.com/mjs527/status/1989787731694940161",
    "timestamp": "2025-11-15T20:08:58.000Z",
    "metadata": {
      "conversation_id": "1989787731694940161",
      "likes": 2,
      "retweets": 0,
      "replies": 0,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989779818582310994",
    "source": "twitter",
    "author": "bentossell",
    "content": "ive not tried handoff so would love to know why that feels great vs using a custom subagent or something else?\n\nwhats oracle? i assume its a specific model with instructions - so i'd say custom slash command or subagent should work - although happy to consider a native /command üòä \n\nwhat are stored threads?",
    "url": "https://x.com/bentossell/status/1989779818582310994",
    "timestamp": "2025-11-15T19:37:31.000Z",
    "metadata": {
      "conversation_id": "1989779818582310994",
      "likes": 0,
      "retweets": 0,
      "replies": 0,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989779083765387330",
    "source": "twitter",
    "author": "bentossell",
    "content": "@constantin_e21 @FactoryAI coming soon",
    "url": "https://x.com/bentossell/status/1989779083765387330",
    "timestamp": "2025-11-15T19:34:36.000Z",
    "metadata": {
      "conversation_id": "1989779083765387330",
      "likes": 2,
      "retweets": 0,
      "replies": 0,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989778892639687094",
    "source": "twitter",
    "author": "bentossell",
    "content": "@brooksjordan @FactoryAI üëÄ",
    "url": "https://x.com/bentossell/status/1989778892639687094",
    "timestamp": "2025-11-15T19:33:50.000Z",
    "metadata": {
      "conversation_id": "1989778892639687094",
      "likes": 1,
      "retweets": 0,
      "replies": 0,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989778846162264206",
    "source": "twitter",
    "author": "bentossell",
    "content": "@OlivierDDR @FactoryAI sorry yeah hit a few of these today, teams fixing!",
    "url": "https://x.com/bentossell/status/1989778846162264206",
    "timestamp": "2025-11-15T19:33:39.000Z",
    "metadata": {
      "conversation_id": "1989778846162264206",
      "likes": 1,
      "retweets": 0,
      "replies": 0,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989777552685277280",
    "source": "twitter",
    "author": "hatchprop",
    "content": "@ChrisVanDerKlau @FactoryAI https://t.co/sV1hCgpKH2",
    "url": "https://x.com/hatchprop/status/1989777552685277280",
    "timestamp": "2025-11-15T19:28:31.000Z",
    "metadata": {
      "conversation_id": "1989777552685277280",
      "likes": 0,
      "retweets": 0,
      "replies": 1,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989776189074846037",
    "source": "twitter",
    "author": "ChrisVanDerKlau",
    "content": "@hatchprop @FactoryAI Proof. What model",
    "url": "https://x.com/ChrisVanDerKlau/status/1989776189074846037",
    "timestamp": "2025-11-15T19:23:06.000Z",
    "metadata": {
      "conversation_id": "1989776189074846037",
      "likes": 0,
      "retweets": 0,
      "replies": 1,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989769667791954354",
    "source": "twitter",
    "author": "DimitriosMitsos",
    "content": "@FactoryAI FactoryAI take a moment to check out this post, this will make sonnet 4.5 -&gt; 4.7 and GPT5.1 to GPT5.3, literally i can send you a preview to check it your selves \nhttps://t.co/Ctufxlykti",
    "url": "https://x.com/DimitriosMitsos/status/1989769667791954354",
    "timestamp": "2025-11-15T18:57:11.000Z",
    "metadata": {
      "conversation_id": "1989769667791954354",
      "likes": 0,
      "retweets": 0,
      "replies": 0,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989768625369313517",
    "source": "twitter",
    "author": "hatchprop",
    "content": "@FactoryAI But the 10B tokens I used this month on codex was $200 / mo",
    "url": "https://x.com/hatchprop/status/1989768625369313517",
    "timestamp": "2025-11-15T18:53:03.000Z",
    "metadata": {
      "conversation_id": "1989768625369313517",
      "likes": 2,
      "retweets": 0,
      "replies": 2,
      "quotes": 0
    }
  },
  {
    "id": "twitter_1989764313037647906",
    "source": "twitter",
    "author": "KirkMarple",
    "content": "@mahaniok Still haven‚Äôt seen any real numbers on these GEO for their customers moving the needle. \n\nWe‚Äôve been able to simulate this just using @FactoryAI Droid and having Claude write new content.  Lower risk and cost.",
    "url": "https://x.com/KirkMarple/status/1989764313037647906",
    "timestamp": "2025-11-15T18:35:54.000Z",
    "metadata": {
      "conversation_id": "1989764313037647906",
      "likes": 4,
      "retweets": 0,
      "replies": 1,
      "quotes": 0
    }
  },
  {
    "id": "github_issue_370",
    "source": "github",
    "author": "aranej",
    "content": "[Issue] UserPromptSubmit hook stdout not injected into LLM context\n\n## Problem\n\nUserPromptSubmit hooks execute successfully but their stdout is not injected into the LLM context, despite [documentation](https://github.com/Factory-AI/factory/blob/main/docs/reference/hooks-reference.mdx#userpromptsubmit-decision-control) stating it should be.\n\n**Expected:** Hook stdout added to context before prompt processing  \n**Actual:** Hook runs, outputs to stdout, but LLM never receives it\n\n## Evidence\n\nDebug logging proves hook execution:\n\n```\n[2025-11-15 16:16:05.140072] U",
    "url": "https://github.com/Factory-AI/factory/issues/370",
    "timestamp": "2025-11-15T15:36:30Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 370
    }
  },
  {
    "id": "github_issue_369",
    "source": "github",
    "author": "BlackCatCmx",
    "content": "[Issue] The .factory/skills directory is created even when \"enableSkills\": false is set\n\nI have set \"enableSkills\": false in the settings.json, but every time droid starts it still automatically creates the .factory/skills directory.\n\nIs this expected behavior (e.g. I misunderstood how to configure it), or is this setting not intended to prevent the creation of the skills directory, or could this be a bug?",
    "url": "https://github.com/Factory-AI/factory/issues/369",
    "timestamp": "2025-11-15T14:38:14Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 369
    }
  },
  {
    "id": "github_issue_366",
    "source": "github",
    "author": "nikolas-sturm",
    "content": "[Issue] Extension not working in WSL\n\nHi, my usual setup consists of VSC connecting to WSL Debian where all my projects reside. The Claude Code extension for VSC works perfectly fine in this setup, however I found that the droid extension is not recognized by the CLI when a WSL workspace is open. It instantly starts working when i open the windows native droid CLI in a non WSL workspace in VSC. I also found that trying to install the extension through the `/settings` menu simply fails when inside WSL. ",
    "url": "https://github.com/Factory-AI/factory/issues/366",
    "timestamp": "2025-11-15T11:29:55Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 366
    }
  },
  {
    "id": "reddit_1oxk7bb",
    "source": "reddit",
    "author": "EggNo4904",
    "content": "Anybody else getting the pretty-print? vercel security etc. dk why it wont accept my card.\n\nThumbnail: self\n",
    "url": "https://www.reddit.com/r/FactoryAi/comments/1oxk7bb/anybody_else_getting_the_prettyprint_vercel/",
    "timestamp": "2025-11-15T06:07:46.000Z",
    "metadata": {
      "score": 1,
      "num_comments": 3,
      "subreddit": "FactoryAi"
    }
  },
  {
    "id": "reddit_1ox98ue",
    "source": "reddit",
    "author": "bentossell",
    "content": "Changelog: v0.26.0\n\n**New features**\n\n‚Ä¢ Session Favorites: New /favorite command to pin/unpin sessions, keeping your active projects at the top of the session list\n\n‚Ä¢ Enhanced Bug Reporting: The /bug command now zips session context, uploads it to Factory, and returns a shareable report ID automatically\n\n‚Ä¢ Cleaner Diff Viewer: Improved UI with horizontal lines instead of borders\n\n\n\n**Bug fixes &amp; smaller items**\n\n‚Ä¢ Always prompt to accept or reject generated specs and pass the correct labels to the UI flow\n\n‚Ä¢ dr",
    "url": "https://www.reddit.com/r/FactoryAi/comments/1ox98ue/changelog_v0260/",
    "timestamp": "2025-11-14T21:33:18.000Z",
    "metadata": {
      "score": 7,
      "num_comments": 7,
      "subreddit": "FactoryAi"
    }
  },
  {
    "id": "github_discussion_361",
    "source": "github",
    "author": "aaronschwartz",
    "content": "[Discussion] Does using the CLI in BYOK mode send data to Factory.ai?\n\nIf we configure the cli to use our own model through BYOK, does any of our data get sent to Factory.ai servers before being submitted to the model for processing? e.g. is there any codebase indexing or embedding being created by Factory.ai in the cloud before prompting the BYOK third party providers?",
    "url": "https://github.com/Factory-AI/factory/discussions/361",
    "timestamp": "2025-11-14T21:17:40Z",
    "metadata": {
      "type": "discussion",
      "category": "Questions",
      "number": 361
    }
  },
  {
    "id": "github_issue_360",
    "source": "github",
    "author": "hanlin-luo",
    "content": "[Issue] CJK double-width characters problem\n\nChinese input (CJK double-width characters) causes cursor misalignment once the input spans more than two lines.\nThis is due to the line editor not correctly implementing Unicode East Asian Width, resulting in incorrect wcwidth calculationsÔºåÔºàmaybeÔºâ.\nIt‚Äôs recommended to enable or fix proper wide-character support in the TUI/line editor being used.\n\nhttps://github.com/user-attachments/assets/7be46a43-0fe3-4e76-98c0-bea3727137e3",
    "url": "https://github.com/Factory-AI/factory/issues/360",
    "timestamp": "2025-11-14T20:31:10Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 360
    }
  },
  {
    "id": "reddit_1ox6p07",
    "source": "reddit",
    "author": "ShipSpecialist6727",
    "content": "Being charge for all limit overage while having credit balance\n\nhttps://preview.redd.it/apq9eqc22a1g1.png?width=713&amp;format=png&amp;auto=webp&amp;s=b55698314d83f7e933e7c750576ab10285a54f3a\n\nhttps://preview.redd.it/c4n0ms032a1g1.png?width=1088&amp;format=png&amp;auto=webp&amp;s=4c124b65440a28c118b9d8a714f0da7618eaf4aa\n\nhttps://preview.redd.it/h2scb4072a1g1.png?width=1226&amp;format=png&amp;auto=webp&amp;s=7fbc4d224122903a737d9f9b4ed0c2092e03ce72\n\n  \nI'm being charge for 40$ limit overage while having my credit balance from my referral .  \nThe support is to",
    "url": "https://www.reddit.com/r/FactoryAi/comments/1ox6p07/being_charge_for_all_limit_overage_while_having/",
    "timestamp": "2025-11-14T19:54:09.000Z",
    "metadata": {
      "score": 2,
      "num_comments": 2,
      "subreddit": "FactoryAi"
    }
  },
  {
    "id": "reddit_1owv5gf",
    "source": "reddit",
    "author": "bentossell",
    "content": "how to set up custom models with Groq in &lt;1 min\n\nhttps://reddit.com/link/1owv5gf/video/vfpxccohu71g1/player\n\nvideo above  \ndocs: [https://docs.factory.ai/cli/byok/groq](https://docs.factory.ai/cli/byok/groq)",
    "url": "https://www.reddit.com/r/FactoryAi/comments/1owv5gf/how_to_set_up_custom_models_with_groq_in_1_min/",
    "timestamp": "2025-11-14T12:27:05.000Z",
    "metadata": {
      "score": 1,
      "num_comments": 0,
      "subreddit": "FactoryAi"
    }
  },
  {
    "id": "reddit_1owpdgs",
    "source": "reddit",
    "author": "Informal-Spinach-345",
    "content": "Todo list with minimax m2\n\nAny one else have issues with Minimax M2 on vllm not being able to properly use the todo list? \n\n  \nThis is what the output looks like, it eventually moves on but I never see a todo list:\n\n   Let me create a comprehensive todo list that captures all the remaining work.\n\n   &lt;/think&gt;\n\n\n\n\n\n\n\n\n\n‚õ¨  &lt;think&gt;I need to fix the todo format - the todos parameter should be an array, not an object.\n\n   &lt;/think&gt;\n\n\n\n\n\n\n\n\n\n‚õ¨  &lt;think&gt;Let me check the todo structure more carefully. Looking",
    "url": "https://www.reddit.com/r/FactoryAi/comments/1owpdgs/todo_list_with_minimax_m2/",
    "timestamp": "2025-11-14T06:41:19.000Z",
    "metadata": {
      "score": 1,
      "num_comments": 6,
      "subreddit": "FactoryAi"
    }
  },
  {
    "id": "reddit_1owbvoj",
    "source": "reddit",
    "author": "bentossell",
    "content": "How should we post changelogs here?\n\nhey, i'm ben - dev rel at factory. you may have seen me answering comments.\n\nwe have a changelog now but has like 4/5 updates a week. its automated as a bot in our discord channel. \n\nshould we add anything here? if so, what format is best?",
    "url": "https://www.reddit.com/r/FactoryAi/comments/1owbvoj/how_should_we_post_changelogs_here/",
    "timestamp": "2025-11-13T20:22:54.000Z",
    "metadata": {
      "score": 7,
      "num_comments": 4,
      "subreddit": "FactoryAi"
    }
  },
  {
    "id": "reddit_1ow4spq",
    "source": "reddit",
    "author": "Rough-Aioli-7829",
    "content": "Why I can't register my email?\n\nImages:\n\thttps://preview.redd.it/rgbm3fcdr11g1.png?auto=webp&amp;s=b0a9871d3d6836057ca15067a94c8a71463ae5ca\n",
    "url": "https://www.reddit.com/r/FactoryAi/comments/1ow4spq/why_i_cant_register_my_email/",
    "timestamp": "2025-11-13T15:58:33.000Z",
    "metadata": {
      "score": 2,
      "num_comments": 1,
      "subreddit": "FactoryAi"
    }
  },
  {
    "id": "reddit_1ow0203",
    "source": "reddit",
    "author": "vargalas",
    "content": "It's the worst onboarding experience ever\n\nThe provided cli instruction doesn't work, curl is complaining about self-signed certificate. The support link (after logging in) doesn't work. I tried to contact the sales on the homepage, but you cannot write them, they will contact. if they want. \n\nSo no Droid for me. I cannot see how they get customers. ",
    "url": "https://www.reddit.com/r/FactoryAi/comments/1ow0203/its_the_worst_onboarding_experience_ever/",
    "timestamp": "2025-11-13T12:42:42.000Z",
    "metadata": {
      "score": 0,
      "num_comments": 3,
      "subreddit": "FactoryAi"
    }
  },
  {
    "id": "github_issue_348",
    "source": "github",
    "author": "aranej",
    "content": "[Issue] üöÄ Feature Request: Native Task Queue Support for Interactive Mode\n\n## üìã Summary\n\nRequest for **native task queuing functionality** in Droid CLI interactive mode, allowing users to queue multiple prompts for sequential or parallel processing without waiting for each task to complete before submitting the next one.\n\n## üéØ Problem Statement\n\n### Current User Experience (Frustrating)\n\n```bash\n> /sys-info is running...\n[User types: /init-prompt]\n‚ùå Command doesn't register - must wait for completion\n\n[After 30 seconds...]\n‚úÖ /sys-info completed\n\n[User must now re-typ",
    "url": "https://github.com/Factory-AI/factory/issues/348",
    "timestamp": "2025-11-13T09:01:07Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 348
    }
  },
  {
    "id": "github_issue_345",
    "source": "github",
    "author": "xianzou",
    "content": "[Issue] Running dorid in PowerShell produces no output\n\n<img width=\"1113\" height=\"626\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/0bddc80a-404e-4fee-9575-96c0cdc89b94\" />\n\nRunning dorid in PowerShell produces no outputÔºåI have already installed dorid.",
    "url": "https://github.com/Factory-AI/factory/issues/345",
    "timestamp": "2025-11-13T01:46:56Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 345
    }
  },
  {
    "id": "reddit_1ovbn7q",
    "source": "reddit",
    "author": "UrAn8",
    "content": "Anyone having issues with the Bridge?\n\nLast few days bridge has been disconnecting frequently. then it started crashing after opening. Restarting cpu didn't help so I deleted the bridge, downloaded it again, restarted my computer, and now it's no longer crashing, but won't reconnect to any old or new sessions. Anyone else having this issue? ",
    "url": "https://www.reddit.com/r/FactoryAi/comments/1ovbn7q/anyone_having_issues_with_the_bridge/",
    "timestamp": "2025-11-12T17:34:16.000Z",
    "metadata": {
      "score": 1,
      "num_comments": 3,
      "subreddit": "FactoryAi"
    }
  },
  {
    "id": "github_issue_341",
    "source": "github",
    "author": "580ai",
    "content": "[Issue] droidÔºöIt seems that the maximum output tokens will not be outputted anymore once they reach 8192\n\nIt will suddenly terminate and then remain stuck without returning any content until the maximum timeout is reached",
    "url": "https://github.com/Factory-AI/factory/issues/341",
    "timestamp": "2025-11-12T08:46:22Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 341
    }
  },
  {
    "id": "github_issue_340",
    "source": "github",
    "author": "korallis",
    "content": "[Issue] Multi Agent Issues in regards to aggrigated RPC\n\n 1. Describe the behavior\n     ‚Ä¢  When you invoke multiple tools via the parallel wrapper, you get no output until every sub-task completes.\n     ‚Ä¢  If any sub-task fails, the entire batch is reported as failed even though the other tasks actually ran.\n\n   2. Explain why it‚Äôs a problem\n     ‚Ä¢  You can‚Äôt see partial results or errors in real time.\n     ‚Ä¢  One failing branch masks useful output from the others, slowing triage and forcing retries.\n\n   3. Request the remedy\n     ‚Ä¢  Ask for streaming",
    "url": "https://github.com/Factory-AI/factory/issues/340",
    "timestamp": "2025-11-11T17:16:42Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 340
    }
  },
  {
    "id": "github_issue_337",
    "source": "github",
    "author": "etwk",
    "content": "[Issue] Make Copy Smooth\n\nDroid UI looks nice, but when I try to copy text to a editor to work further on it, there are some formatting issues:\n  - Frequent un-intended line breaks\n  - Empty space before each lines\n\nCan we make the transaction more smooth? It'll be huge UX enhancement.",
    "url": "https://github.com/Factory-AI/factory/issues/337",
    "timestamp": "2025-11-10T23:07:38Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 337
    }
  },
  {
    "id": "github_issue_335",
    "source": "github",
    "author": "etwk",
    "content": "[Issue] Subagent No Output\n\nTesting some subagents with the builtin models, the subagents seems running well but the main process never gets the output from the subagent task.\n\nUsing latest Droid (0.22.14) with the GPT-5-Codex model, getting this result:\n```\nAttempted to invoke the available subagents (code-reviewer, debugger) with simple greeting prompts, but each Task call returned ‚ÄúNo output received from task subagent,‚Äù so the requested ‚Äúhi‚Äù response could not be produced.\n```",
    "url": "https://github.com/Factory-AI/factory/issues/335",
    "timestamp": "2025-11-10T12:55:34Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 335
    }
  },
  {
    "id": "github_issue_334",
    "source": "github",
    "author": "etwk",
    "content": "[Issue] droid exec will change default model for the main process\n\nCurrently droid exec will change default model for the main process, maybe we should change this partly because of convenience, and partly because of process separation, not letting 2 unrelated process interfering in any possible way.",
    "url": "https://github.com/Factory-AI/factory/issues/334",
    "timestamp": "2025-11-10T12:42:52Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 334
    }
  },
  {
    "id": "github_issue_331",
    "source": "github",
    "author": "spionkind",
    "content": "[Issue] Issue with GPT5 Codex and Custom Model on Droid\n\n Hi,\n\nwhenever I chosed GPT5-codex model it show this error message:\nError: 400 Invalid 'tools[40].name': string too long. Expected a string with maximum length 64, but got a string with length 66 instead.\n\nand when I try with custom model (qwen3 coder), it shows this:\nError: 400 <400> InternalError.Algo.InvalidParameter: The length of the tool name cannot exceed 64.\n\nDroid still works well with other model suchas sonnet 4.5, glm 4.6\n\nPlease help me on this issues\n\nBest regards,\nGiang",
    "url": "https://github.com/Factory-AI/factory/issues/331",
    "timestamp": "2025-11-09T16:19:30Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 331
    }
  },
  {
    "id": "github_issue_330",
    "source": "github",
    "author": "olsavmic",
    "content": "[Issue] Feature Request: Implement Statusline Support\n\n  Add support for customizable statuslines in Factory's CLI interface, as documented in [Claude Code's statusline\n  feature](https://code.claude.com/docs/en/statusline).\n\n  Motivation\n\n  Statuslines provide persistent contextual information at the bottom of the interface, which is incredibly helpful for:\n   ‚Ä¢  Git awareness: Immediately see the current branch without running git status\n   ‚Ä¢  Directory context: Know your current working directory at a glance\n   ‚Ä¢  Session info: Display the curren",
    "url": "https://github.com/Factory-AI/factory/issues/330",
    "timestamp": "2025-11-09T11:34:21Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 330
    }
  },
  {
    "id": "reddit_1osf7sj",
    "source": "reddit",
    "author": "LittleChallenge8717",
    "content": "Custom Droid | Web search Task failed\n\nWeb search and calling custom droid feature in Droid not working, I use claude code inside droid (with CLIProxyAPI),\n\n(Neither works on openrouter models)\n\n\"\n\nWEB SEARCH  (\"current bitcoin price BTC USD\")\n\nSearch failed: Error: Error performing web search: Fetch failed\n\nWEB SEARCH  (\"ethereum ETH price USD current\")\n\nSearch failed: Error: Error performing web search: Fetch failed\n\n\"\n\n\" TASK  (mermaid-expert: \"Create system architecture diagram\")\n\n‚ö† Task failed\n\nTASK  (seo-meta-optimizer: \"Optimi",
    "url": "https://www.reddit.com/r/FactoryAi/comments/1osf7sj/custom_droid_web_search_task_failed/",
    "timestamp": "2025-11-09T09:35:38.000Z",
    "metadata": {
      "score": 3,
      "num_comments": 3,
      "subreddit": "FactoryAi"
    }
  },
  {
    "id": "github_issue_329",
    "source": "github",
    "author": "TheSingular",
    "content": "[Issue] Spec mode hides reasoning level effort in model name\n\nWhen using the spec mode, you cannot see the reasoning effort of the model you have selected (if it supports reasoning).\nYou can still change the reasoning effort via the tab key, but you cannot see its effect at all. Other modes properly display the reasoning effort in parentheses.\n\nHow to reproduce:\n1. Use model Sonnet 4.5 with low reasoning effort, do not choose a separate model for the spec model.\n2. Toggle to spec mode via ctrl + t.\n3. Observe the parentheses containing the reasoning effort",
    "url": "https://github.com/Factory-AI/factory/issues/329",
    "timestamp": "2025-11-09T09:12:43Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 329
    }
  },
  {
    "id": "github_issue_328",
    "source": "github",
    "author": "a112121788",
    "content": "[Issue] custom model support reasoning\n\n",
    "url": "https://github.com/Factory-AI/factory/issues/328",
    "timestamp": "2025-11-08T15:47:20Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 328
    }
  },
  {
    "id": "github_issue_325",
    "source": "github",
    "author": "Evan-Kim2028",
    "content": "[Issue] Task subagent runs tools but returns no final message (intermittent)\n\nSummary\n- Intermittently, Task executions succeed and run tools (Grep/Read/Glob/TodoWrite) but no final assistant message is emitted, and the wrapper surfaces: \"No output received from task subagent.\" This blocks using dbt droids for reviews.\n\nEnvironment\n- OS: macOS darwin 24.6.0\n- Factory CLI/session: latest as of 2025-11-07\n- Model: OpenAI GPT-5\n- gh: 2.80.0\n\nReproduction (today)\n1) Launch Task with subagent_type=\"dbt-quality-droid\" on local repo /Users/evandekim/Documents/spellbook (branch: ",
    "url": "https://github.com/Factory-AI/factory/issues/325",
    "timestamp": "2025-11-07T21:38:02Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 325
    }
  },
  {
    "id": "github_issue_321",
    "source": "github",
    "author": "nilzzzzzz",
    "content": "[Issue] Droid exec --output-format debug is not returning the result\n\nWe rely heavily on `droid exec` but since one of the last updates there is a huge issue. Running `droid exec --output-format debug` does not return anymore the result but just the debug outputs before:\n\nReproduction:\n\n```\ndroid exec \"which services does the backend have\" --output-format debug\n\n{\"type\":\"system\",\"subtype\":\"init\",\"cwd\":\"/Users/nilz/Repositories/awork-backend\",\"session_id\":\"273c1ae2-ffec-4ef5-b748-e04d713000fa\",\"tools\":[\"Read\",\"LS\",\"Execute\",\"Edit\",\"ApplyPatch\",\"Grep\",\"Glob\",\"Create",
    "url": "https://github.com/Factory-AI/factory/issues/321",
    "timestamp": "2025-11-07T12:11:44Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 321
    }
  },
  {
    "id": "github_issue_319",
    "source": "github",
    "author": "Bladed3d",
    "content": "[Issue] \"Endless Processing\" When using custom model openrouter: Grok-Code-Fast-1\n\nWhen using custom model openrouter: Grok-Code-Fast-1 after giving it a task, Droid will keep working on that until I press ESC then display context it has been building up. Upon further review, it seems that Droid using Grok is having a conversation with itself, forgetting to include me and wait for my actual participation:\n\n(This is just some of the context...)\n\nConfirming Dashboard Functionality\n   ‚Ä¢  The dashboard no longer shows duplicates, displaying only the current session's 1\n      root ",
    "url": "https://github.com/Factory-AI/factory/issues/319",
    "timestamp": "2025-11-06T20:19:04Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 319
    }
  },
  {
    "id": "github_issue_314",
    "source": "github",
    "author": "lifeisnphard",
    "content": "[Issue] File selector '@' not working in iTerm2\n\n## Environment\n- **OS**: macOS (darwin 25.0.0)\n- **Terminal**: iTerm2\n- **Factory CLI**: (version needs verification)\n- **Date**: 2025-11-04\n\n## Issue Description\nThe file selector feature triggered by the `@` symbol is not working in iTerm2. This functionality works correctly in Warp terminal but fails to appear in iTerm2.\n\n## Expected Behavior\nWhen typing `@` in the Factory Droid CLI interface, a file selector/picker UI should appear to allow interactive file selection, similar to the behavior",
    "url": "https://github.com/Factory-AI/factory/issues/314",
    "timestamp": "2025-11-04T19:12:02Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 314
    }
  },
  {
    "id": "github_discussion_309",
    "source": "github",
    "author": "msneijders",
    "content": "[Discussion] Droid CLI: configurable timeouts per mcp server\n\n**Motivation**: I use a 'Language Server Protocol' mcp for adding tools to query symbols and find_usage and such. But on large projects and during first-time use they can take quite some time before giving responses; causing timeouts in droid cli. If I could configure a larger timeout for this specific mcp server it could work better.",
    "url": "https://github.com/Factory-AI/factory/discussions/309",
    "timestamp": "2025-11-04T10:48:14Z",
    "metadata": {
      "type": "discussion",
      "category": "Feature requests",
      "number": 309
    }
  },
  {
    "id": "github_issue_308",
    "source": "github",
    "author": "Evan-Kim2028",
    "content": "[Issue] MCP servers configured globally aren‚Äôt exposed inside custom droid sessions (tools work in main interface, fail inside droid)\n\n## Summary\nMCP servers (dbt-mcp, igloo-mcp, github-mcp) are configured globally and work in the main interface, but their tools are not available when running inside a custom droid session. As a result, dbt/igloo/github MCP tool calls succeed outside the droid but fail inside the droid with messages indicating no MCP interface/tools.\n\n## Environment\n- OS: macOS 14.6 (24.6.0)\n- Shell: droid CLI (interactive)\n- Custom droid: `dbt_quality_droid` (personal droid)\n- MCP: servers registered globally (",
    "url": "https://github.com/Factory-AI/factory/issues/308",
    "timestamp": "2025-11-03T20:08:35Z",
    "metadata": {
      "type": "issue",
      "labels": [],
      "number": 308
    }
  },
  {
    "id": "github_discussion_240",
    "source": "github",
    "author": "factory-ben",
    "content": "[Discussion] Mini Changelog\n\n<img width=\"686\" height=\"397\" alt=\"Screenshot 2025-10-22 at 14 58 16\" src=\"https://github.com/user-attachments/assets/0288d87c-55b1-44b4-91cb-33d745fc052b\" />\r\n",
    "url": "https://github.com/Factory-AI/factory/discussions/240",
    "timestamp": "2025-10-23T09:45:14Z",
    "metadata": {
      "type": "discussion",
      "category": "Factory Official",
      "number": 240
    }
  },
  {
    "id": "github_discussion_225",
    "source": "github",
    "author": "JohnDotOwl",
    "content": "[Discussion] Change Log\n\nCan we have more updated change logs somewhere if possible. \r\n\r\nSuccessfully updated to version 0.21.5 Just saw this ",
    "url": "https://github.com/Factory-AI/factory/discussions/225",
    "timestamp": "2025-10-21T09:52:33Z",
    "metadata": {
      "type": "discussion",
      "category": "Feature requests",
      "number": 225
    }
  },
  {
    "id": "github_discussion_223",
    "source": "github",
    "author": "JohnDotOwl",
    "content": "[Discussion] [Discussion] Droid Exec Headless Streaming with output format (JSON)\n\nDroid Exec (Headless)\r\nIt make sense to use --output-format json if you're running a headless droid exec as that's the best way to programmatically manage the headless droid. The issue for me is i can't see the process and potentially stop it without --output-format debug and i can't run both output format. \r\n\r\nShould we consider having a different parameter for \"streaming\" , it's not so much for debugging but more to manage the task itself. ",
    "url": "https://github.com/Factory-AI/factory/discussions/223",
    "timestamp": "2025-10-21T02:46:01Z",
    "metadata": {
      "type": "discussion",
      "category": "Feature requests",
      "number": 223
    }
  },
  {
    "id": "github_discussion_214",
    "source": "github",
    "author": "0xStuart",
    "content": "[Discussion] JetBrains Plugin\n\nThe documentation says there is a JetBrains Plugin in the marketplace, but I can't find it.",
    "url": "https://github.com/Factory-AI/factory/discussions/214",
    "timestamp": "2025-10-19T19:55:35Z",
    "metadata": {
      "type": "discussion",
      "category": "Questions",
      "number": 214
    }
  },
  {
    "id": "github_discussion_164",
    "source": "github",
    "author": "tgerighty",
    "content": "[Discussion] Droid Exec - custom models supported?\n\ncan we have support for custom models in droid exec? even if it were just simple endpoint/key.",
    "url": "https://github.com/Factory-AI/factory/discussions/164",
    "timestamp": "2025-10-10T18:34:32Z",
    "metadata": {
      "type": "discussion",
      "category": "Feature requests",
      "number": 164
    }
  },
  {
    "id": "github_discussion_151",
    "source": "github",
    "author": "snowarch",
    "content": "[Discussion] Sound or notification when completed task\n\nIt would be useful to have some kind of sound or notification in the system when the agent has completed the task. Droid CLI, Arch Linux.",
    "url": "https://github.com/Factory-AI/factory/discussions/151",
    "timestamp": "2025-10-09T10:20:14Z",
    "metadata": {
      "type": "discussion",
      "category": "Feature requests",
      "number": 151
    }
  },
  {
    "id": "github_discussion_108",
    "source": "github",
    "author": "factory-ben",
    "content": "[Discussion] Release v1.3.220\n\n---\r\n\r\nThis release brings HTTP MCP server support, a `/compress` command to optimize your context usage and droid exec for CDE.\r\n\r\n---\r\n\r\n## HTTP MCP Server Support\r\n\r\nFactory now supports streamable HTTP-based Model Context Protocol (MCP) servers in addition to stdio-based servers. This enables you to connect to MCP servers running as web services, making it easier to integrate with cloud-hosted tools and services.\r\n\r\nWhen adding an MCP server, Factory automatically detects whether it's HTTP o",
    "url": "https://github.com/Factory-AI/factory/discussions/108",
    "timestamp": "2025-10-03T20:21:56Z",
    "metadata": {
      "type": "discussion",
      "category": "Factory Official",
      "number": 108
    }
  },
  {
    "id": "github_discussion_99",
    "source": "github",
    "author": "Evan-Kim2028",
    "content": "[Discussion] Droid source code\n\nIs droid closed source? I am not able to find any code. I would like to be able to see how the droid tools are implemented if the code is available to see",
    "url": "https://github.com/Factory-AI/factory/discussions/99",
    "timestamp": "2025-10-02T17:51:27Z",
    "metadata": {
      "type": "discussion",
      "category": "Questions",
      "number": 99
    }
  },
  {
    "id": "github_discussion_95",
    "source": "github",
    "author": "factory-ben",
    "content": "[Discussion] CLI v0.16.0 Release Notes\n\n- Sonnet 4.5 model is now available\r\n- Clear chat input using double ESC or Ctrl-C\r\n- User-configured allow and deny list of commands to execute\r\n- Option to disable droid commit co-authoring\r\n- Auth using FACTORY_API_KEY\r\n- Fixed UI bugs due to resizing",
    "url": "https://github.com/Factory-AI/factory/discussions/95",
    "timestamp": "2025-10-01T20:05:01Z",
    "metadata": {
      "type": "discussion",
      "category": "Factory Official",
      "number": 95
    }
  },
  {
    "id": "github_discussion_92",
    "source": "github",
    "author": "msneijders",
    "content": "[Discussion] MCP tools filtering\n\nI would like to configure exactly what tools of a MCP server should be made available.\r\n\r\nMotivation: MCP servers with overlapping functionality and too many unneeded tools is confusing the models.",
    "url": "https://github.com/Factory-AI/factory/discussions/92",
    "timestamp": "2025-10-01T09:07:33Z",
    "metadata": {
      "type": "discussion",
      "category": "Feature requests",
      "number": 92
    }
  },
  {
    "id": "github_discussion_82",
    "source": "github",
    "author": "mweichert",
    "content": "[Discussion] Web API?\n\nHi there! Is there a web API to create and manage sessions?",
    "url": "https://github.com/Factory-AI/factory/discussions/82",
    "timestamp": "2025-09-29T13:16:25Z",
    "metadata": {
      "type": "discussion",
      "category": "Questions",
      "number": 82
    }
  },
  {
    "id": "github_discussion_72",
    "source": "github",
    "author": "rubendn",
    "content": "[Discussion] Droid selection in CLI?\n\nIs there a way to use one of the specialized Droids (Product, Tutorial, etc) via the CLI?  I don't see a way to select in the slash commands.\r\n\r\nAlso, is there a way on Windows to select a local workspace path besides ~\\?\r\n\r\nThanks!",
    "url": "https://github.com/Factory-AI/factory/discussions/72",
    "timestamp": "2025-09-29T05:17:49Z",
    "metadata": {
      "type": "discussion",
      "category": "Questions",
      "number": 72
    }
  },
  {
    "id": "github_discussion_66",
    "source": "github",
    "author": "factory-ben",
    "content": "[Discussion] 2 new community builds: use claude/codex max subscriptions with droid\n\n- [Factory CLI with ChatGPT Codex / Claude subscription via CLIProxyAPI](https://gist.github.com/chandika/c4b64c5b8f5e29f6112021d46c159fdd) - Guide to run Factory CLI against Claude Code Max or ChatGPT Codex through CLIProxyAPI by [chandika](https://github.com/chandika)\r\n- [Factory CLI with Claude subscription via CLIProxyAPI](https://gist.github.com/ben-vargas/9f1a14ac5f78d10eba56be437b7c76e5) - Setup instructions for using Factory CLI with Claude Code Max through CLIProxyAPI by [ben-vargas](ht",
    "url": "https://github.com/Factory-AI/factory/discussions/66",
    "timestamp": "2025-09-27T19:15:40Z",
    "metadata": {
      "type": "discussion",
      "category": "Show + tell",
      "number": 66
    }
  },
  {
    "id": "github_discussion_65",
    "source": "github",
    "author": "jimkyndemeyer",
    "content": "[Discussion] BYOK with Anthropic models on Amazon Bedrock\n\nWould be great to have the option of using Amazon Bedrock for Anthropic models with BYOK.\r\n\r\nAnthropic has official SDKs for Bedrock, but the model env config is a bit different:\r\n- AWS region\r\n- Access key id\r\n- Secret access key",
    "url": "https://github.com/Factory-AI/factory/discussions/65",
    "timestamp": "2025-09-27T09:46:55Z",
    "metadata": {
      "type": "discussion",
      "category": "Feature requests",
      "number": 65
    }
  },
  {
    "id": "github_discussion_59",
    "source": "github",
    "author": "factory-ben",
    "content": "[Discussion] #1 on Terminal Bench\n\n<img width=\"1582\" height=\"972\" alt=\"Screenshot 2025-09-25 at 17 15 56\" src=\"https://github.com/user-attachments/assets/9e9c80b6-825c-41cc-b8c7-236b21e3f61c\" />\r\n\r\nRead how we did it: https://factory.ai/news/terminal-bench",
    "url": "https://github.com/Factory-AI/factory/discussions/59",
    "timestamp": "2025-09-25T19:22:25Z",
    "metadata": {
      "type": "discussion",
      "category": "Factory Official",
      "number": 59
    }
  },
  {
    "id": "github_discussion_32",
    "source": "github",
    "author": "factory-ben",
    "content": "[Discussion] CLI v0.7.0 Release Notes\n\nCLI Release (0.7.0)\r\n\r\n- Autonomy level selection: Users can now select any preferred autonomy level when confirming the ExitSpec tool\r\n- Claude reasoning support: Added support for Claude's reasoning capabilities\r\n- Session resumption: Users can now resume existing sessions when operating in Exec Mode\r\n- Image reading capabilities: The CLI can now read and process images",
    "url": "https://github.com/Factory-AI/factory/discussions/32",
    "timestamp": "2025-09-19T20:15:48Z",
    "metadata": {
      "type": "discussion",
      "category": "Factory Official",
      "number": 32
    }
  },
  {
    "id": "github_discussion_25",
    "source": "github",
    "author": "DannyAziz",
    "content": "[Discussion] Change model even if I've already written a prompt out\n\nSometimes I write a message, then realise I want to drop down to Sonnet, and I have to delete my message to hit the slash command. I wish there was a way to do this without deleting everything.",
    "url": "https://github.com/Factory-AI/factory/discussions/25",
    "timestamp": "2025-09-16T22:58:36Z",
    "metadata": {
      "type": "discussion",
      "category": "Feature requests",
      "number": 25
    }
  },
  {
    "id": "github_discussion_20",
    "source": "github",
    "author": "factory-ben",
    "content": "[Discussion] CLI v0.5.0 Release Notes\n\nv0.5 release:\r\n\r\n- Users can now queue messages while the Droid works\r\n- Added clearer autonomy level & mode descriptions\r\n- Fixed many flickering issues\r\n- Revamped `--help` command\r\n- Automatic VSCode / Cursor / Windsurf CLI detection,\r\n- Windows delete key bug fixed\r\n- Lots of small UI improvements\r\n- Lots of quality of life and bug fixes\n\ncoming soon:\n- custom droids/subagents\n- notif sounds\n- much more\n\nany requests? ",
    "url": "https://github.com/Factory-AI/factory/discussions/20",
    "timestamp": "2025-09-16T06:27:29Z",
    "metadata": {
      "type": "discussion",
      "category": "Factory Official",
      "number": 20
    }
  }
]